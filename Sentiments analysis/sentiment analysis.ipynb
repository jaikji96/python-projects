{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('E:/dataset/sentiment analysis/train_2kmZucJ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3      0  We love this! Would you go? #talk #makememorie...\n",
       "3   4      0  I'm wired I know I'm George I was made that wa...\n",
       "4   5      1  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('E:/dataset/sentiment analysis/test_oJQbWVk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>I hate the new #iphone upgrade. Won't let me d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>currently shitting my fucking pants. #apple #i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>I'd like to puts some CD-ROMS on my iPad, is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>My ipod is officially dead. I lost all my pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>Been fighting iTunes all night! I only want th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              tweet\n",
       "0  7921  I hate the new #iphone upgrade. Won't let me d...\n",
       "1  7922  currently shitting my fucking pants. #apple #i...\n",
       "2  7923  I'd like to puts some CD-ROMS on my iPad, is t...\n",
       "3  7924  My ipod is officially dead. I lost all my pict...\n",
       "4  7925  Been fighting iTunes all night! I only want th..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1953 entries, 0 to 1952\n",
      "Data columns (total 2 columns):\n",
      "id       1953 non-null int64\n",
      "tweet    1953 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 30.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "5       1\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      1\n",
       "11      1\n",
       "12      1\n",
       "13      0\n",
       "14      1\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      1\n",
       "19      1\n",
       "20      0\n",
       "21      0\n",
       "22      1\n",
       "23      1\n",
       "24      0\n",
       "25      0\n",
       "26      1\n",
       "27      1\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "7890    1\n",
       "7891    0\n",
       "7892    0\n",
       "7893    0\n",
       "7894    0\n",
       "7895    1\n",
       "7896    1\n",
       "7897    1\n",
       "7898    0\n",
       "7899    0\n",
       "7900    0\n",
       "7901    1\n",
       "7902    1\n",
       "7903    0\n",
       "7904    1\n",
       "7905    0\n",
       "7906    0\n",
       "7907    0\n",
       "7908    1\n",
       "7909    0\n",
       "7910    0\n",
       "7911    0\n",
       "7912    0\n",
       "7913    1\n",
       "7914    0\n",
       "7915    0\n",
       "7916    0\n",
       "7917    0\n",
       "7918    0\n",
       "7919    0\n",
       "Name: label, Length: 7920, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7920 entries, 0 to 7919\n",
      "Data columns (total 3 columns):\n",
      "id       7920 non-null int64\n",
      "label    7920 non-null int64\n",
      "tweet    7920 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 185.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>iPhone software update fucked up my phone big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>hey #apple when you make a new ipod dont make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Ha! Not heavy machinery but it does what I nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Contemplating giving in to the iPhone bandwago...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                              tweet\n",
       "4    5      1  What amazing service! Apple won't even talk to...\n",
       "5    6      1  iPhone software update fucked up my phone big ...\n",
       "10  11      1  hey #apple when you make a new ipod dont make ...\n",
       "11  12      1  Ha! Not heavy machinery but it does what I nee...\n",
       "12  13      1  Contemplating giving in to the iPhone bandwago..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['label']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5894\n",
       "1    2026\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x206ce2eaf60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_train = df_train['tweet'].str.len()\n",
    "length_test  = df_test['tweet'].str.len()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(length_train,bins=20,label = \"TRAIN_TWEET\")\n",
    "plt.hist(length_test,bins =20, label = \"TEST_TWEET\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the length of the test tweet in the dataframe is very much less as compared to the train tweet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text.\n",
    "\n",
    "Before we begin cleaning, let’s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = df_train.append(df_test,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...\n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...\n",
       "4   5    1.0  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>9869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>9870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Now Available - Hoodie. Check it out here - ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>9871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There goes a crack right across the screen. If...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>9872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@codeofinterest as i said #Adobe big time we m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>9873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finally I got it .. thanx my father .. #Samsun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label                                              tweet\n",
       "9868  9869    NaN  #SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...\n",
       "9869  9870    NaN  Now Available - Hoodie. Check it out here - ht...\n",
       "9870  9871    NaN  There goes a crack right across the screen. If...\n",
       "9871  9872    NaN  @codeofinterest as i said #Adobe big time we m...\n",
       "9872  9873    NaN  Finally I got it .. thanx my father .. #Samsun..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt,pattern):\n",
    "    r=re.findall(pattern,input_txt)\n",
    "    \n",
    "    for i in r:\n",
    "        input_txt=re.sub(i,'',input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi['clean_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  #fingerprint #Pregnancy Test https://goo.gl/h1...  \n",
       "1  Finally a transparant silicon case ^^ Thanks t...  \n",
       "2  We love this! Would you go? #talk #makememorie...  \n",
       "3  I'm wired I know I'm George I was made that wa...  \n",
       "4  What amazing service! Apple won't even talk to...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>#fingerprint #Pregnancy Test https   goo gl h ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>Finally a transparant silicon case    Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>We love this  Would you go  #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>I m wired I know I m George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>What amazing service  Apple won t even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  #fingerprint #Pregnancy Test https   goo gl h ...  \n",
       "1  Finally a transparant silicon case    Thanks t...  \n",
       "2  We love this  Would you go  #talk #makememorie...  \n",
       "3  I m wired I know I m George I was made that wa...  \n",
       "4  What amazing service  Apple won t even talk to...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation and special character.\n",
    "combi['clean_tweet'] = combi['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "combi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing short words\n",
    "combi['clean_tweet']=combi['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>#fingerprint #Pregnancy Test https MfQV #andro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>Finally transparant silicon case Thanks uncle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>love this Would #talk #makememories #unplug #r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>wired know George made that #iphone #cute #dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>What amazing service Apple even talk about que...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  #fingerprint #Pregnancy Test https MfQV #andro...  \n",
       "1  Finally transparant silicon case Thanks uncle ...  \n",
       "2  love this Would #talk #makememories #unplug #r...  \n",
       "3  wired know George made that #iphone #cute #dav...  \n",
       "4  What amazing service Apple even talk about que...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [#fingerprint, #Pregnancy, Test, https, MfQV, ...\n",
       "1    [Finally, transparant, silicon, case, Thanks, ...\n",
       "2    [love, this, Would, #talk, #makememories, #unp...\n",
       "3    [wired, know, George, made, that, #iphone, #cu...\n",
       "4    [What, amazing, service, Apple, even, talk, ab...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenisation\n",
    "\n",
    "tokenised_tweet = combi['clean_tweet'].apply(lambda x:x.split())\n",
    "\n",
    "tokenised_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming with porter stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "toketweetStem = tokenised_tweet.apply(lambda x:[stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [#fingerprint, #pregnanc, test, http, mfqv, #a...\n",
       "1    [final, transpar, silicon, case, thank, uncl, ...\n",
       "2    [love, thi, would, #talk, #makememori, #unplug...\n",
       "3    [wire, know, georg, made, that, #iphon, #cute,...\n",
       "4    [what, amaz, servic, appl, even, talk, about, ...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toketweetStem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also checking snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_snow = SnowballStemmer(\"english\")\n",
    "\n",
    "toketweetSnow = tokenised_tweet.apply(lambda x:[stemmer_snow.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [#fingerprint, #pregnanc, test, https, mfqv, #...\n",
       "1    [final, transpar, silicon, case, thank, uncl, ...\n",
       "2    [love, this, would, #talk, #makememori, #unplu...\n",
       "3    [wire, know, georg, made, that, #iphon, #cute,...\n",
       "4    [what, amaz, servic, appl, even, talk, about, ...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toketweetSnow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#fingerprint',\n",
       " '#pregnanc',\n",
       " 'test',\n",
       " 'https',\n",
       " 'mfqv',\n",
       " '#android',\n",
       " '#app',\n",
       " '#beauti',\n",
       " '#cute',\n",
       " '#health',\n",
       " '#iger',\n",
       " '#iphoneon',\n",
       " '#iphonesia',\n",
       " '#iphon']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toketweetSnow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#fingerprint',\n",
       " '#pregnanc',\n",
       " 'test',\n",
       " 'http',\n",
       " 'mfqv',\n",
       " '#android',\n",
       " '#app',\n",
       " '#beauti',\n",
       " '#cute',\n",
       " '#health',\n",
       " '#iger',\n",
       " '#iphoneonli',\n",
       " '#iphonesia',\n",
       " '#iphon']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toketweetStem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>#fingerprint #pregnanc test https mfqv #androi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>final transpar silicon case thank uncl #yay #s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>love this would #talk #makememori #unplug #rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>wire know georg made that #iphon #cute #davent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>what amaz servic appl even talk about question...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  #fingerprint #pregnanc test https mfqv #androi...  \n",
       "1  final transpar silicon case thank uncl #yay #s...  \n",
       "2  love this would #talk #makememori #unplug #rel...  \n",
       "3  wire know georg made that #iphon #cute #davent...  \n",
       "4  what amaz servic appl even talk about question...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets use our snowball stemmer tokenised tweet\n",
    "\n",
    "for i in range(len(toketweetSnow)):\n",
    "    toketweetSnow[i]=' '.join(toketweetSnow[i])\n",
    "    \n",
    "combi['clean_tweet'] = toketweetSnow\n",
    "\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG-of-WORDS FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to analyse a preprocessed data, it need to converted into features.\n",
    "#using bag-of-words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer= CountVectorizer(max_df=0.90,min_df=2,max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9873, 1000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = bow_vectorizer.fit_transform(combi['clean_tweet'])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIDF Features\n",
    "\n",
    "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus.\n",
    "\n",
    "TF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n",
    "\n",
    "Let’s have a look at the important terms related to TF-IDF:\n",
    "\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    "IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "\n",
    "TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9873, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000,stop_words='english')\n",
    "tfidf=tfidf_vectorizer.fit_transform(combi['clean_tweet'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "Word embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like King -man +woman = Queen, which is mind-blowing.\n",
    "\n",
    "The advantages of using word embeddings over BOW or TF-IDF are:\n",
    "\n",
    "Dimensionality reduction - significant reduction in the no. of features required to build a model.\n",
    "\n",
    "It capture meanings of the words, semantic relationships and the different types of contexts they are used in.\n",
    "\n",
    "\n",
    "Word2Vec Embeddings\n",
    "Word2Vec is not a single algorithm but a combination of two techniques – CBOW (Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.\n",
    "\n",
    "CBOW tends to predict the probability of a word given a context. A context may be a single adjacent word or a group of surrounding words. The Skip-gram model works in the reverse manner, it tries to predict the context for a given word.\n",
    "\n",
    "Below is a diagrammatic representation of a 1-word context window Word2Vec model.\n",
    "\n",
    "\n",
    "There are three laters: - an input layer, - a hidden layer, and - an output layer.\n",
    "\n",
    "The input layer and the output, both are one- hot encoded of size [1 X V], where V is the size of the vocabulary (no. of unique words in the corpus). The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. The weights learned by the model are then used as the word-vectors.\n",
    "\n",
    "We will go ahead with the Skip-gram model as it has the following advantages:\n",
    "\n",
    "It can capture two semantics for a single word. i.e it will have two vector representations of ‘apple’. One for the company Apple and the other for the fruit.\n",
    "\n",
    "Skip-gram with negative sub-sampling outperforms CBOW generally.\n",
    "\n",
    "We will train a Word2Vec model on our data to obtain vector representations for all the unique words present in our corpus. There is one more option of using pre-trained word vectors instead of training our own model. Some of the freely available pre-trained vectors are:\n",
    "\n",
    "Google News Word Vectors\n",
    "\n",
    "Freebase names\n",
    "\n",
    "DBPedia vectors (wiki2vec)\n",
    "\n",
    "However, for this course, we will train our own word vectors since size of the pre-trained word vectors is generally huge.\n",
    "\n",
    "Let’s train a Word2Vec model on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_again = combi['clean_tweet'].apply(lambda x:x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokenised_again,size=200,window=5,\n",
    "                                  min_count=2,\n",
    "                                  sg=1,hs=0,negative=10,\n",
    "                                  workers=2,\n",
    "                                  seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1846007, 2567180)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(tokenised_again,total_examples=len(combi['clean_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hamburg', 0.8193708658218384),\n",
       " ('diamond', 0.8035350441932678),\n",
       " ('macaron', 0.7979682683944702),\n",
       " ('pattern', 0.7906177639961243),\n",
       " ('totem', 0.7816594839096069),\n",
       " ('#affirm', 0.7707458734512329),\n",
       " ('rainbow', 0.7643883228302002),\n",
       " ('laser', 0.7501481771469116),\n",
       " ('henna', 0.7438300848007202),\n",
       " ('bestcheapphon', 0.7386386394500732)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.most_similar(positive=\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.22995289,  0.64209867,  0.03517272, -0.02807054,  0.1914678 ,\n",
       "       -0.06931217,  0.31573468, -0.2591764 , -0.17968985, -0.2940012 ,\n",
       "       -0.20642361,  0.22839044,  0.25718653,  0.14738743, -0.13481519,\n",
       "       -0.19939092,  0.1311853 , -0.02296082,  0.51283723, -0.15798211,\n",
       "       -0.28987536,  0.54819167, -0.12780473,  0.13452856,  0.3769379 ,\n",
       "        0.12477472, -0.39689896, -0.22677657,  0.13568471, -0.35764775,\n",
       "        0.4049188 , -0.29155838, -0.30774307,  0.10043439,  0.03572094,\n",
       "       -0.27481252, -0.44231424,  0.13431324,  0.15215957,  0.05254231,\n",
       "        0.21445419,  0.08985748,  0.24212146,  0.45672905,  0.06886608,\n",
       "       -0.41277978, -0.25951147, -0.404326  , -0.29283294,  0.11542978,\n",
       "        0.00930906,  0.46962953, -0.03430159, -0.5474187 , -0.0647354 ,\n",
       "        0.09652998,  0.00740897, -0.06852113,  0.15307476,  0.02402356,\n",
       "        0.08478185, -0.01854081, -0.41364437,  0.17194062,  0.22005707,\n",
       "       -0.04035107,  0.5470895 ,  0.04794438,  0.01753961, -0.25676125,\n",
       "        0.2545142 , -0.12830152,  0.26159635,  0.1567696 ,  0.10705933,\n",
       "        0.21897638,  0.04999011,  0.20154259, -0.1033513 ,  0.08801406,\n",
       "        0.22676693,  0.19350319,  0.00557362, -0.1622674 , -0.07766528,\n",
       "       -0.4389239 ,  0.02659942, -0.6795792 ,  0.18541165,  0.00944536,\n",
       "       -0.05574895,  0.30054545, -0.11265436,  0.28826553,  0.04919638,\n",
       "        0.05810774, -0.1813518 , -0.14630677,  0.57233095,  0.7373911 ,\n",
       "        0.43525   ,  0.5188039 , -0.30973598,  0.6094614 ,  0.5431957 ,\n",
       "       -0.30153733,  0.29736143, -0.02190263,  0.08044419,  0.26618907,\n",
       "       -0.03346765,  0.35943902,  0.10802706, -0.2591225 , -0.6966624 ,\n",
       "       -0.00667953, -0.17310438,  0.15442719,  0.14825542, -0.6011552 ,\n",
       "       -0.08941931,  0.15160611,  0.09792119, -0.06999603, -0.0148221 ,\n",
       "        0.19548602, -0.16003351, -0.01635801,  0.47296467, -0.35052687,\n",
       "        0.07819378,  0.43630108,  0.14164537, -0.08915961, -0.21471356,\n",
       "       -0.45176896, -0.36310485, -0.64264005,  0.90255827, -0.10637975,\n",
       "       -0.18683289, -0.3800836 , -0.2458224 ,  0.00948331, -0.1152016 ,\n",
       "        0.02787023, -0.35403684,  0.27908662,  0.09004178,  0.4791714 ,\n",
       "        0.16477664,  0.07767135,  0.12075285, -0.13581304, -0.25942197,\n",
       "       -0.21993224, -0.07748905, -0.07877649, -0.40252867,  0.00393501,\n",
       "        0.516955  ,  0.2042972 ,  0.31724653,  0.41683385,  0.37842807,\n",
       "        0.31955174,  0.08743005, -0.44451383,  0.5443245 , -0.32627177,\n",
       "        0.18745477, -0.4711246 ,  0.05644239,  0.31478837,  0.12276191,\n",
       "       -0.14567038,  0.04097367, -0.06421283,  0.37217596, -0.30685192,\n",
       "        0.06273812, -0.38641018,  0.27305168, -0.5463045 , -0.06231545,\n",
       "       -0.23004578,  0.26453134, -0.5862049 ,  0.34080204,  0.32611957,\n",
       "        0.07470861,  0.04024639,  0.41016185,  0.55197054,  0.52374375,\n",
       "        0.4505852 , -0.09214552,  0.5054421 , -0.4650209 ,  0.25879335],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v['pattern']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens,size):\n",
    "    vec= np.zeros(size).reshape((1,size))\n",
    "    count=0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec+=model_w2v[word].reshape((1,size))\n",
    "            count +=1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count !=0:\n",
    "        vec /=count\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "wordvec_arrays =np.zeros((len(tokenised_again),200))\n",
    "for i in range(len(tokenised_again)):\n",
    "    wordvec_arrays[i,:]=word_vector(tokenised_again[i],200)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "    wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9873, 200)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the bof and tf-idf features contains our data set have the feature of the 1000  and where as word2svec data set hav the features of the 200 only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output =[]\n",
    "    for i, s in zip(twt.index,twt):\n",
    "        output.append(LabeledSentence(s,[\"tweet_\"+str(i)]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = add_label(tokenised_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['#fingerprint', '#pregnanc', 'test', 'https', 'mfqv', '#android', '#app', '#beauti', '#cute', '#health', '#iger', '#iphoneon', '#iphonesia', '#iphon'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['final', 'transpar', 'silicon', 'case', 'thank', 'uncl', '#yay', '#soni', '#xperia', '#sonyexperia', 'http', 'instagram', 'yget'], tags=['tweet_1']),\n",
       " LabeledSentence(words=['love', 'this', 'would', '#talk', '#makememori', '#unplug', '#relax', '#iphon', '#smartphon', '#wifi', '#connect', 'http', 'lsupcu'], tags=['tweet_2']),\n",
       " LabeledSentence(words=['wire', 'know', 'georg', 'made', 'that', '#iphon', '#cute', '#daventri', '#home', 'http', 'instagr'], tags=['tweet_3']),\n",
       " LabeledSentence(words=['what', 'amaz', 'servic', 'appl', 'even', 'talk', 'about', 'question', 'have', 'unless', 'them', 'their', 'stupid', 'support'], tags=['tweet_4']),\n",
       " LabeledSentence(words=['iphon', 'softwar', 'updat', 'fuck', 'phone', 'time', 'stupid', 'iphon'], tags=['tweet_5'])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "#lets train doc2vec model\n",
    "\n",
    "model_2vec = gensim.models.Doc2Vec(dm=1,\n",
    "                                  dm_mean=1,\n",
    "                                  size=200,\n",
    "                                  window=5,\n",
    "                                  negative=7,\n",
    "                                  min_count=5,\n",
    "                                  workers=3,\n",
    "                                  alpha=0.1,\n",
    "                                  seed =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 9873/9873 [00:00<00:00, 1584357.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model_2vec.build_vocab([i for i in tqdm(labeled_tweets)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2vec.train(labeled_tweets,total_examples=len(combi['clean_tweet']),epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing doc2vec Features set\n",
    "\n",
    "docvec_arrays = np.zeros((len(tokenised_again),200))\n",
    "\n",
    "for i in range(len(combi)):\n",
    "    docvec_arrays[i,:] = model_2vec.docvecs[i].reshape((1,200))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_df= pd.DataFrame(docvec_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9873, 200)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling our train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using logistic regression on the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting train and test set from the bow features varname bow\n",
    "train_bow = bow[:7920,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = bow[7920:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spiliting the data into training and validation set\n",
    "Xtrain_bow,X_val_bow,y_train_bow,y_val_bow = train_test_split(train_bow,df_train['label'],random_state=42,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression()\n",
    "lreg.fit(Xtrain_bow,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lreg.predict_proba(X_val_bow)# prediction on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_int = prediction[:,1] >=0.3 #if prediction is greater than or equal to 0.3 than 1 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_int =  prediction_int.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8033589923023093"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_bow,prediction_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we get the accuracy of the 80% \n",
    "\n",
    "now submission file creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:,1]>=0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "df_test['label'] = test_pred_int\n",
    "submission = df_test[['id','label']]\n",
    "submission.to_csv('sub_logis_reg.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidf[:7920,:]\n",
    "test_tfidf=tfidf[7920:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = train_tfidf[y_train_bow.index]\n",
    "X_Val_tfidf = train_tfidf[y_val_bow.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(X_train_tfidf,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log = lreg.predict_proba(X_Val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_int_log = predict_log[:,1]>=0.3\n",
    "predict_int_log = predict_int_log.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8139372822299651"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_bow,predict_int_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now it is better than above bow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = wordvec_df.iloc[:7920,:]\n",
    "test_w2v = wordvec_df.iloc[7920:,:]\n",
    "\n",
    "X_train_w2v = train_w2v.iloc[y_train_bow.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_w2v = train_w2v.iloc[y_val_bow.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(X_train_w2v,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_w2v = lreg.predict_proba(X_valid_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_w2v_int = prediction_w2v[:,1] >=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_w2v_int = prediction_w2v_int.astype(np.int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821008984105045"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_bow,prediction_w2v_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy is greater than previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d2v = docvec_df.iloc[:7920,:]\n",
    "test_d2v = docvec_df.iloc[7920:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d2v = train_d2v.iloc[y_train_bow.index,:]\n",
    "X_valid_d2v = train_d2v.iloc[y_val_bow.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(X_train_d2v,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_d2v = lreg.predict_proba(X_valid_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_d2v_int = prediction_d2v[:,1]>=0.3\n",
    "\n",
    "prediction_d2v_int = prediction_d2v_int.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6319396847155586"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val_bow,prediction_d2v_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very bad Performance in this doc2vector features.\n",
    "\n",
    "\n",
    "# Highest feature extraction is word2vectors scoring 81% in the f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying xboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
