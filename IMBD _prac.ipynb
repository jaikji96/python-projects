{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/dataset/imdb/train.csv',names=['ratings','reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just don't get some of the big premises of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Please note that I haven't seen the film since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>First off, let me say that I am a great believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>OK i own this DVD i got it new at amazon... i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews\n",
       "0        0  I just don't get some of the big premises of t...\n",
       "1        0  Please note that I haven't seen the film since...\n",
       "2        0  I went for this movie believing it had good ra...\n",
       "3        0  First off, let me say that I am a great believ...\n",
       "4        0  OK i own this DVD i got it new at amazon... i ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings                                                    0\n",
       "reviews    I just don't get some of the big premises of t...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "ratings    25000 non-null int64\n",
      "reviews    25000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>1</td>\n",
       "      <td>This was another great Tom Berenger movie.. Bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>1</td>\n",
       "      <td>This movie is really not all that bad. But the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>1</td>\n",
       "      <td>This is not a \"loose\", but a precise, faithful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>1</td>\n",
       "      <td>MULHOLLAND DRIVE made me the definitive fan of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>1</td>\n",
       "      <td>I just saw this movie today with my children (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ratings                                            reviews\n",
       "12500        1  This was another great Tom Berenger movie.. Bu...\n",
       "12501        1  This movie is really not all that bad. But the...\n",
       "12502        1  This is not a \"loose\", but a precise, faithful...\n",
       "12503        1  MULHOLLAND DRIVE made me the definitive fan of...\n",
       "12504        1  I just saw this movie today with my children (..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ratings']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was another great Tom Berenger movie.. But some people are right it was like another SGT BARNES character but it was still awsome.. Tom Berenger played a great sniper in the jungles Of Panama! Billy Zane was a wuss at first just like Cpl Upham from Saving Private Ryan but then he got a little more aggresive in the end! Sniper was awsome and action buffs should watch it.. I remind you it wouldnt have as much action as a reg action flick.. i got this one on DVD too and it is excellent!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[12500][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I just don't get some of the big premises of this episode - that Miranda is so remarkable, and that there's anything so ugly it would make you insane. Someone here made the remark that maybe it's the frequency of the light waves or something rather than it being ugliness. Miranda is just a jerk. The episode is slow, inconsistent and way too talky. I also don't quite understand why Kolos is an ambassador - why doesn't the Federation just leave the damn Medusans be? There's one part I do like, when Kolos is speaking through Spock about the loneliness of the human experience. Overall, I love TOS and even at its lamest, I'll always tune in. This episode though - mmm, I wouldn't purchase it except for a used copy under $3.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFTER reading some of the reviews it is has been clear that the rating 0 is negative reviews and 1 is positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: ratings, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ratings'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we can say that 50% of the reviews are bad and 50% of reviews are positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('E:/dataset/imdb/test.csv',names = ['ratings','reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sometimes I think that somewhere in the \"Lifet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>As a flagship show, Attack of the Show (AOTS) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I was so looking forward to seeing this when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely the worst experience I have ever be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Ladies and Gentlemen.. Be sad (or be glad !).....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews\n",
       "0        0  Sometimes I think that somewhere in the \"Lifet...\n",
       "1        0  As a flagship show, Attack of the Show (AOTS) ...\n",
       "2        0  I was so looking forward to seeing this when i...\n",
       "3        0  Absolutely the worst experience I have ever be...\n",
       "4        0  Ladies and Gentlemen.. Be sad (or be glad !)....."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAHWdJREFUeJzt3Xt0VeWd//H3V1CCFgoEqgzBSRA6lYuAhIuDiBd+3GwFtRS1FobSUqtOqbadgi5/orazcMbVWqotRWV+2KLR0iIs1KmiouCFW42AAk0UigGrEQhFC2Lk+/vjPAknkDyQnJPkBD6vtbLO3s9+9j7f84ScD/ty9jF3R0REpCYnNXYBIiKS2RQUIiISpaAQEZEoBYWIiEQpKEREJEpBISIiUQoKERGJUlCIiEiUgkJERKKaN3YBddW+fXvPzc1t7DJERJqMtWvXfujuHWq7XpMNitzcXNasWdPYZYiINBlm9te6rKdDTyIiEqWgEBGRKAWFiIhENdlzFCKSGT799FNKSkrYv39/Y5ciQVZWFjk5OZx88slp2Z6CQkRSUlJSQqtWrcjNzcXMGrucE567s3PnTkpKSsjLy0vLNnXoSURSsn//frKzsxUSGcLMyM7OTusenoJCRFKmkMgs6f59KChERCRK5yhEJK1ypz2Z1u1tnXlpWrcntaegqI0Zn09h3T3pq0NEKpWVlfHII49w/fXX12q90aNH88gjj9CmTZt6qqx6s2fP5tRTT2XChAkN+ryp0KEnEWnSysrK+NWvfnVE+2effRZd76mnnko5JMrLy2u9znXXXdekQgIUFCLSxE2bNo23336bPn360L9/fy666CKuueYaevXqBcDYsWPp168fPXr0YM6cOZXr5ebm8uGHH7J161bOPvtsvv3tb9OjRw+GDx/Ovn37any+Cy+8kFtuuYWhQ4fyi1/8gtLSUq688kr69+9P//79efnllzl48CC5ubmUlZVVrte1a1fef/99ZsyYwT333APA22+/zciRI+nXrx9Dhgxh06ZNfPbZZ3Tp0gV3p6ysjJNOOomXXnoJgCFDhlBcXMyLL75Inz596NOnD3379mXv3r31MbSVdOhJRJq0mTNnsmHDBgoLC1m2bBmXXnopGzZsqPwMwdy5c2nXrh379u2jf//+XHnllWRnZ1fZRlFREY8++igPPPAAX/va1/jDH/7AtddeW+NzlpWV8eKLLwJwzTXXcNNNN3H++eezbds2RowYwcaNGxkzZgwLFy5k0qRJrFy5ktzcXE4//fQq25kyZQqzZ8+mW7durFy5kuuvv57nn3+eL37xi7z11lts2bKFfv36sXz5cgYOHEhJSQldu3blpptu4v7772fw4MF89NFHZGVlpXlUq1JQiMhxZcCAAVU+aDZr1iwWLlwIwLvvvktRUdERQZGXl0efPn0A6NevH1u3bo0+x/jx4yunly5dyltvvVU5//e//529e/cyfvx47rzzTiZNmkRBQUGVdQA++ugjXnnlFcaNG1fZ9sknnwCJPYeXXnqJLVu2MH36dB544AGGDh1K//79ARg8eDA333wzX//617niiivIyck51uGpEx16EpHjymmnnVY5vWzZMpYuXcqrr77KG2+8Qd++fav9IFqLFi0qp5s1a3bUcw/Jz3Hw4EFeffVVCgsLKSwsZPv27bRq1YrzzjuP4uJiSktLeeKJJ7jiiiuqbOPgwYO0adOmcr3CwkI2btwIJIJi+fLlrFq1itGjR1NWVsayZcu44IILgMThtgcffJB9+/YxaNAgNm3aVPuBqgXtUYhIWjX05aytWrWq8Rj9nj17aNu2LaeeeiqbNm3itddeS/vzDx8+nPvuu48f/ehHABQWFtKnTx/MjMsvv5ybb76Zs88++4i9mNatW5OXl8fvf/97xo0bh7uzbt06evfuzcCBA5kwYQJdunQhKyuLPn368Jvf/IYlS5YAiXMbvXr1olevXrz66qts2rSJL33pS2l/bRW0RyEiTVp2djaDBw+mZ8+elW/WFUaOHEl5eTnnnHMOt912G4MGDUr788+aNYs1a9Zwzjnn0L17d2bPnl25bPz48fzud7874rBThfnz5/PQQw/Ru3dvevTowaJFi4DEHk7nzp0r6x0yZAh79+6tPEF/77330rNnT3r37k3Lli0ZNWpU2l9XMnP3en2C+pKfn+8N/g13+hyFyBE2btzI2Wef3dhlyGGq+72Y2Vp3z6/ttrRHISIiUTpHISJSjRtuuIGXX365StvUqVOZNGlSI1XUeBQUIiLVuP/++xu7hIyhQ08iIhKloBARkSgFhYiIROkchYikVyqXkVe7PV1a3ti0RyEiTdbOnTsr76J6xhln0KlTp8r5AwcOHPN25s6dy9/+9rd6rBRuvfVWXnjhhXp9jvqiPQoRabKys7MpLCwEYMaMGXzuc5/jhz/8Ya23M3fuXM4991zOOOOMY+pfXl5O8+a1e/v86U9/Wuu6MoX2KETkuDRv3jwGDBhAnz59uP766zl48CDl5eV84xvfoFevXvTs2ZNZs2bx2GOPUVhYyPjx46N7Ijk5Odx1110MHjyYhQsXUlRUxIgRI+jXrx8XXHABf/nLX9i1axd5eXlU3PHio48+4swzz6S8vJxrr72WJ554AoDVq1czdOhQ+vXrx6hRo3j//ffZsWMHAwcOBGDt2rWYGTt27AASd7fdv38/BQUFlbfuuOiiixpgFBO0RyEix50NGzawcOFCXnnlFZo3b86UKVMoKCjgrLPO4sMPP2T9+vVA4nsl2rRpwy9/+Uvuu+++yluN1+S0006r/BDeRRddxIMPPshZZ53Fyy+/zI033sgzzzxD9+7dWbFiBUOGDGHRokWMHj26yt7HJ598wtSpU1m8eDHt27dn/vz53HbbbcyZM4c9e/bw8ccfs3z5cvLz8ysfc3JyyMrK4o477mDZsmWcfvrpVb4Uqb4pKETkuLN06VJWr15Nfn7itkb79u2jc+fOjBgxgs2bNzN16lRGjx7N8OHDa7Xdipv7lZWV8dprr3HllVdWLqu4Nfn48eN57LHHGDJkCAUFBdx8881VtrFx40befPNNhg0bBiS+srXi+yTOO+88XnnlFZYvX84tt9zC0qVL2bdvH0OGDAES30MxYcIExo0bd8Rty+vTUQ89mdlcM/vAzDYktbUzs2fNrCg8tg3tZmazzKzYzNaZ2blJ60wM/YvMbGJSez8zWx/WmWVmlu4XKSInFnfnm9/8ZuX3PGzevJnbbruN7Oxs1q1bx/nnn8+sWbP4zne+U6vtVnwPhbvTvn37Kt8lsWFD4i1y7NixLFmyhJ07d7J+/XqGDh16RG3nnHNO5Xrr16/n6aefBg59YdH27dv5yle+wuuvv86KFSsqv4figQce4I477mDr1q307t2b3bt3pzpUx+RY9ij+H3Af8HBS2zTgOXefaWbTwvyPgVFAt/AzEPg1MNDM2gG3A/mAA2vNbLG77w59pgCvAU8BI4GnU39pmSV32pNAw9+rX6TBZcDlrMOGDeOrX/0qU6dOpX379uzcuZOPP/6Yli1bkpWVxbhx48jLy+O6664D4t9pUZ22bdvSsWNHFi5cyOWXX87BgwdZv349vXv3pnXr1vTt25fvf//7XHbZZZx0UtX/j3fv3p3t27ezatUqBgwYwIEDBygqKqJHjx5ccMEFzJgxg4svvpjmzZvTqlUrnnnmGX72s58B8M477zBo0CAGDhzI4sWL2b59O23btk3fwNXgqEHh7i+ZWe5hzWOAC8P0PGAZiaAYAzzsiTM5r5lZGzPrGPo+6+67AMzsWWCkmS0DWrv7q6H9YWAsx2FQiEjD6dWrF7fffjvDhg3j4MGDnHzyycyePZtmzZoxefJk3B0z4+677wZg0qRJfOtb36Jly5asWrWKU0455ajPUVBQwHe/+11mzJjBgQMHuPbaa+nduzeQOPx09dVXs2LFiiPWa9GiBQsWLOB73/see/fupby8nB/84Af06NGDrl27Ul5eXrkHMXjwYEpLS2ndujUAN910E1u2bMHdGT58OD179kzXkEUd0/dRhKBY4u49w3yZu7dJWr7b3dua2RJgpruvCO3PkQiQC4Esd/9JaL8N2EciYGa6+7DQPgT4sbt/uYY6ppDY++DMM8/s99e//rUOLzkFKXyQKHf/I4D2KOT4o++jyEyZ/H0U1Z1f8Dq0V8vd57h7vrvnd+jQoY4liohIbdT1qqf3zayju78XDi19ENpLgM5J/XKAHaH9wsPal4X2nGr6i4g0issuu4xt27ZVabvnnnsqr1I6EdU1KBYDE4GZ4XFRUvuNZlZA4mT2nhAmfwL+s+LqKGA4MN3dd5nZXjMbBKwEJgC/rGNNItJIKo75Hw8WL17c2CWkLN1fcX3UoDCzR0nsDbQ3sxISVy/NBB43s8nANmBc6P4UMBooBv4BTApF7zKzu4DVod+dFSe2ge+SuLKqJYmT2DqRLdKEZGVlsXPnTrKzs4+bsGjK3J2dO3eSlZWVtm0ey1VPV9ew6JJq+jpwQw3bmQvMraZ9DdAwp+5FJO1ycnIoKSmhtLS0sUuRICsrq/JDfOmgT2aLSEpOPvlk8vLyGrsMqUe6KaCIiEQpKEREJEpBISIiUQoKERGJUlCIiEiUgkJERKIUFCIiEqWgEBGRKAWFiIhEKShERCRKQSEiIlEKChERiVJQiIhIlIJCRESiFBQiIhKloBARkSgFhYiIRCkoREQkSkEhIiJRCgoREYlSUIiISJSCQkREohQUIiISpaAQEZEoBYWIiEQpKEREJCqloDCzm8zsTTPbYGaPmlmWmeWZ2UozKzKzx8zslNC3RZgvDstzk7YzPbRvNrMRqb0kERFJpzoHhZl1Ar4H5Lt7T6AZcBVwN/Bzd+8G7AYmh1UmA7vdvSvw89APM+se1usBjAR+ZWbN6lqXiIikV6qHnpoDLc2sOXAq8B5wMbAgLJ8HjA3TY8I8YfklZmahvcDdP3H3LUAxMCDFukREJE3qHBTuvh24B9hGIiD2AGuBMncvD91KgE5huhPwbli3PPTPTm6vZh0REWlkqRx6aktibyAP+CfgNGBUNV29YpUaltXUXt1zTjGzNWa2prS0tPZFi4hIraVy6GkYsMXdS939U+CPwL8CbcKhKIAcYEeYLgE6A4Tlnwd2JbdXs04V7j7H3fPdPb9Dhw4plC4iIscqlaDYBgwys1PDuYZLgLeAF4Cvhj4TgUVhenGYJyx/3t09tF8VrorKA7oBq1KoS0RE0qj50btUz91XmtkC4M9AOfA6MAd4Eigws5+EtofCKg8BvzWzYhJ7EleF7bxpZo+TCJly4AZ3/6yudYmISHrVOSgA3P124PbDmt+hmquW3H0/MK6G7fwU+GkqtYiISP3QJ7NFRCRKQSEiIlEKChERiVJQiIhIlIJCRESiFBQiIhKloBARkSgFhYiIRKX0gbumKnfak3Vab2tWmgsREWkCtEchIiJRCgoREYlSUIiISJSCQkREohQUIiISpaAQEZEoBYWIiEQpKEREJEpBISIiUQoKERGJUlCIiEiUgkJERKIUFCIiEqWgEBGRKAWFiIhEKShERCRKQSEiIlEKChERiVJQiIhIVEpBYWZtzGyBmW0ys41mdp6ZtTOzZ82sKDy2DX3NzGaZWbGZrTOzc5O2MzH0LzKziam+KBERSZ9U9yh+Afyvu38J6A1sBKYBz7l7N+C5MA8wCugWfqYAvwYws3bA7cBAYABwe0W4iIhI46tzUJhZa+AC4CEAdz/g7mXAGGBe6DYPGBumxwAPe8JrQBsz6wiMAJ51913uvht4FhhZ17pERCS9Utmj6AKUAv9jZq+b2YNmdhpwuru/BxAevxD6dwLeTVq/JLTV1H4EM5tiZmvMbE1paWkKpYuIyLFKJSiaA+cCv3b3vsDHHDrMVB2rps0j7Uc2us9x93x3z+/QoUNt6xURkTpIJShKgBJ3XxnmF5AIjvfDISXC4wdJ/TsnrZ8D7Ii0i4hIBqhzULj734B3zexfQtMlwFvAYqDiyqWJwKIwvRiYEK5+GgTsCYem/gQMN7O24ST28NAmIiIZoHmK6/87MN/MTgHeASaRCJ/HzWwysA0YF/o+BYwGioF/hL64+y4zuwtYHfrd6e67UqxLRETSJKWgcPdCIL+aRZdU09eBG2rYzlxgbiq1iIhI/Uh1j6JJ2pp1TWOXICLSZOgWHiIiEqWgEBGRKAWFiIhEKShERCRKQSEiIlEKChERiVJQiIhIlIJCRESiFBQiIhKloBARkSgFhYiIRCkoREQkSkEhIiJRCgoREYlSUIiISJSCQkREohQUIiISpaAQEZEoBYWIiEQpKEREJEpBISIiUQoKERGJUlCIiEiUgkJERKIUFCIiEqWgEBGRqJSDwsyamdnrZrYkzOeZ2UozKzKzx8zslNDeIswXh+W5SduYHto3m9mIVGsSEZH0SccexVRgY9L83cDP3b0bsBuYHNonA7vdvSvw89APM+sOXAX0AEYCvzKzZmmoS0RE0iCloDCzHOBS4MEwb8DFwILQZR4wNkyPCfOE5ZeE/mOAAnf/xN23AMXAgFTqEhGR9El1j+Je4D+Ag2E+Gyhz9/IwXwJ0CtOdgHcBwvI9oX9lezXriIhII6tzUJjZl4EP3H1tcnM1Xf0oy2LrHP6cU8xsjZmtKS0trVW9IiJSN6nsUQwGLjOzrUABiUNO9wJtzKx56JMD7AjTJUBngLD888Cu5PZq1qnC3ee4e76753fo0CGF0kVE5FjVOSjcfbq757h7LomT0c+7+9eBF4Cvhm4TgUVhenGYJyx/3t09tF8VrorKA7oBq+pal4iIpFfzo3eptR8DBWb2E+B14KHQ/hDwWzMrJrEncRWAu79pZo8DbwHlwA3u/lk91CUiInVgif/UNz35+fm+Zs2auq084/PpLaY+zdjT2BWIyHHCzNa6e35t19Mns0VEJKo+Dj1JGuVOe7JyeuvMSxuxEhE5UWmPQkREohQUIiISpaAQEZEoBYWIiEQpKEREJEpBISIiUQoKERGJUlCIiEiUgkJERKIUFCIiEqWgEBGRKAWFiIhEKShERCRKQSEiIlEKChERiVJQiIhIlIJCRESiFBQiIhKloBARkSgFhYiIRCkoREQkSkEhIiJRCgoREYlSUIiISJSCQkREohQUIiISVeegMLPOZvaCmW00szfNbGpob2dmz5pZUXhsG9rNzGaZWbGZrTOzc5O2NTH0LzKziam/LBERSZdU9ijKgR+4+9nAIOAGM+sOTAOec/duwHNhHmAU0C38TAF+DYlgAW4HBgIDgNsrwkVERBpfnYPC3d9z9z+H6b3ARqATMAaYF7rNA8aG6THAw57wGtDGzDoCI4Bn3X2Xu+8GngVG1rUuERFJr7ScozCzXKAvsBI43d3fg0SYAF8I3ToB7yatVhLaamqv7nmmmNkaM1tTWlqajtJFROQoUg4KM/sc8Afg++7+91jXato80n5ko/scd8939/wOHTrUvlgREam1lILCzE4mERLz3f2Pofn9cEiJ8PhBaC8BOietngPsiLSLiEgGaF7XFc3MgIeAje7+s6RFi4GJwMzwuCip/UYzKyBx4nqPu79nZn8C/jPpBPZwYHpd6zrebM265tDMjFqsOGNPuksRkRNUnYMCGAx8A1hvZoWh7RYSAfG4mU0GtgHjwrKngNFAMfAPYBKAu+8ys7uA1aHfne6+K4W6REQkjeocFO6+gurPLwBcUk1/B26oYVtzgbl1rUWOlDvtySrzW2de2kiViEhTp09mi4hIlIJCRESiFBQiIhKloBARkSgFhYiIRCkoREQkSkEhIiJRCgoREYlSUIiISJSCQkREohQUIiISpaAQEZEoBYWIiESlcptxyWBVvscCavVdFrn7H6m6Ld15VuSEpj0KERGJUlCIiEiUgkJERKIUFCIiEqWgEBGRKAWFiIhEKShERCRKn6OQI9T1MxiHf/4C9BkMkeOB9ihERCRKQSEiIlEKChERidI5CkmbI85tQErnN0DnOEQygfYoREQkKmP2KMxsJPALoBnwoLvPbOSSpAFVuzcCtbrrba3N2FOPGxc5fmREUJhZM+B+4P8AJcBqM1vs7m81bmVyPMud9mSNy3TIS+SQTDn0NAAodvd33P0AUACMaeSaRESEDNmjADoB7ybNlwADG6kWOUHUeLgL4oe8dMhKTjCZEhRWTZsf0clsCjAlzH5kZpvr8FztgQ/rsF5jamo1N7V6oTY131HdP9cGd3yPcWZoavXC0Wv+57psNFOCogTonDSfA+w4vJO7zwHmpPJEZrbG3fNT2UZDa2o1N7V6oenV3NTqhaZXc1OrF+qv5kw5R7Ea6GZmeWZ2CnAVsLiRaxIRETJkj8Ldy83sRuBPJC6PnevubzZyWSIiQoYEBYC7PwU81QBPldKhq0bS1GpuavVC06u5qdULTa/mplYv1FPN5n7EOWMREZFKmXKOQkREMtQJFRRmNtLMNptZsZlNa8Q6OpvZC2a20czeNLOpob2dmT1rZkXhsW1oNzObFepeZ2bnJm1rYuhfZGYT67nuZmb2upktCfN5ZrYyPPdj4UIEzKxFmC8Oy3OTtjE9tG82sxH1XG8bM1tgZpvCWJ+XyWNsZjeFfw8bzOxRM8vKtDE2s7lm9oGZbUhqS9uYmlk/M1sf1pllZilfi1xDzf8d/l2sM7OFZtYmaVm141fT+0dNv6N01pu07Idm5mbWPsw3zBi7+wnxQ+Ik+dtAF+AU4A2geyPV0hE4N0y3Av4CdAf+C5gW2qcBd4fp0cDTJD5vMghYGdrbAe+Ex7Zhum091n0z8AiwJMw/DlwVpmcD3w3T1wOzw/RVwGNhunsY9xZAXvh9NKvHeucB3wrTpwBtMnWMSXzodAvQMmls/y3Txhi4ADgX2JDUlrYxBVYB54V1ngZG1VPNw4HmYfrupJqrHT8i7x81/Y7SWW9o70zigp+/Au0bcozr5Q80E3/CwPwpaX46ML2x6wq1LCJxn6vNQMfQ1hHYHKZ/A1yd1H9zWH418Juk9ir90lxjDvAccDGwJPwj+zDpj61yfMM/5vPCdPPQzw4f8+R+9VBvaxJvvHZYe0aOMYfuTtAujNkSYEQmjjGQS9U33bSMaVi2Kam9Sr901nzYssuB+WG62vGjhveP2N9BuusFFgC9ga0cCooGGeMT6dBTdbcJ6dRItVQKhwz6AiuB0939PYDw+IXQrabaG/I13Qv8B3AwzGcDZe5eXs1zV9YVlu8J/Ruy3i5AKfA/ljhc9qCZnUaGjrG7bwfuAbYB75EYs7Vk9hhXSNeYdgrTh7fXt2+S+J81R6mtuvbY30HamNllwHZ3f+OwRQ0yxidSUBzTbUIakpl9DvgD8H13/3usazVtHmlPKzP7MvCBu689hppiyxryd9CcxO77r929L/AxicMiNWnsMW5L4kaYecA/AacBoyLPnQljfDS1rbHBazezW4FyYH5FUw01NFrNZnYqcCvwf6tbXMu66lTviRQUx3SbkIZiZieTCIn57v7H0Py+mXUMyzsCH4T2mmpvqNc0GLjMzLaSuLPvxST2MNqYWcVncZKfu7KusPzzwK4GrLeihhJ3XxnmF5AIjkwd42HAFncvdfdPgT8C/0pmj3GFdI1pSZg+vL1ehBO8Xwa+7uE4TB1q/pCaf0fpchaJ/0C8Ef4Gc4A/m9kZdai3bmOczmOXmfxD4n+Y74QBrzgZ1aORajHgYeDew9r/m6onBf8rTF9K1RNWq0J7OxLH4duGny1Au3qu/UIOncz+PVVP4l0fpm+g6onWx8N0D6qeKHyH+j2ZvRz4lzA9I4xvRo4xibslvwmcGmqYB/x7Jo4xR56jSNuYkridzyAOnWgdXU81jwTeAjoc1q/a8SPy/lHT7yid9R62bCuHzlE0yBjX2xtKJv6QuELgLySuXri1Ees4n8Tu3jqgMPyMJnG88zmgKDxW/GKNxBc7vQ2sB/KTtvVNoDj8TGqA2i/kUFB0IXEFRXH4Y2kR2rPCfHFY3iVp/VvD69hMGq5oOUqtfYA1YZyfCH8wGTvGwB3AJmAD8NvwZpVRYww8SuIcyqck/nc6OZ1jCuSH1/82cB+HXYyQxpqLSRzDr/j7m3208aOG94+afkfprPew5Vs5FBQNMsb6ZLaIiESdSOcoRESkDhQUIiISpaAQEZEoBYWIiEQpKEREJEpBISIiUQoKERGJUlCIiEjU/wcM7z4Yqgz2EAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_train= df['reviews'].str.len()\n",
    "length_test= df_test['reviews'].str.len()\n",
    "\n",
    "plt.hist(length_train,bins=20,label=\"train_reviews\")\n",
    "plt.hist(length_test, bins = 20, label=\"Test_reviews\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = df.append(df_test,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just don't get some of the big premises of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Please note that I haven't seen the film since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>First off, let me say that I am a great believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>OK i own this DVD i got it new at amazon... i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews\n",
       "0        0  I just don't get some of the big premises of t...\n",
       "1        0  Please note that I haven't seen the film since...\n",
       "2        0  I went for this movie believing it had good ra...\n",
       "3        0  First off, let me say that I am a great believ...\n",
       "4        0  OK i own this DVD i got it new at amazon... i ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "# to remove the unwanted text pattern from the reviews.\n",
    "\n",
    "def remove_pattern(input_text,pattern):\n",
    "    re =  re.findall(pattern, input_text)\n",
    "    for i in r:\n",
    "        input_text = re.sub(i,'', input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**removing some unwanted pattern like characters and hashtag and spaces with the help of regular expression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi['clean_review'] =  combi['reviews'].str.replace(\"[^a-zA-Z#]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just don't get some of the big premises of t...</td>\n",
       "      <td>I just don t get some of the big premises of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Please note that I haven't seen the film since...</td>\n",
       "      <td>Please note that I haven t seen the film since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>First off, let me say that I am a great believ...</td>\n",
       "      <td>First off  let me say that I am a great believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>OK i own this DVD i got it new at amazon... i ...</td>\n",
       "      <td>OK i own this DVD i got it new at amazon    i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews  \\\n",
       "0        0  I just don't get some of the big premises of t...   \n",
       "1        0  Please note that I haven't seen the film since...   \n",
       "2        0  I went for this movie believing it had good ra...   \n",
       "3        0  First off, let me say that I am a great believ...   \n",
       "4        0  OK i own this DVD i got it new at amazon... i ...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  I just don t get some of the big premises of t...  \n",
       "1  Please note that I haven t seen the film since...  \n",
       "2  I went for this movie believing it had good ra...  \n",
       "3  First off  let me say that I am a great believ...  \n",
       "4  OK i own this DVD i got it new at amazon    i ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**removing short words from the clean_review series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi['clean_review'] = combi['clean_review'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just don't get some of the big premises of t...</td>\n",
       "      <td>just some premises this episode that Miranda r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Please note that I haven't seen the film since...</td>\n",
       "      <td>Please note that haven seen film since discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "      <td>went this movie believing good ratings Firstly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>First off, let me say that I am a great believ...</td>\n",
       "      <td>First that great believer Fanpro stuff continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>OK i own this DVD i got it new at amazon... i ...</td>\n",
       "      <td>this amazon mean think badass pretty cool flic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews  \\\n",
       "0        0  I just don't get some of the big premises of t...   \n",
       "1        0  Please note that I haven't seen the film since...   \n",
       "2        0  I went for this movie believing it had good ra...   \n",
       "3        0  First off, let me say that I am a great believ...   \n",
       "4        0  OK i own this DVD i got it new at amazon... i ...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  just some premises this episode that Miranda r...  \n",
       "1  Please note that haven seen film since discove...  \n",
       "2  went this movie believing good ratings Firstly...  \n",
       "3  First that great believer Fanpro stuff continu...  \n",
       "4  this amazon mean think badass pretty cool flic...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Normalisation and applying PorterStemmer function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [just, some, premises, this, episode, that, Mi...\n",
       "1    [Please, note, that, haven, seen, film, since,...\n",
       "2    [went, this, movie, believing, good, ratings, ...\n",
       "3    [First, that, great, believer, Fanpro, stuff, ...\n",
       "4    [this, amazon, mean, think, badass, pretty, co...\n",
       "Name: clean_review, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_reviews = combi['clean_review'].apply(lambda x: x.split()) #tokenising\n",
    "tokenised_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_reviews = tokenised_reviews.apply(lambda x: [stemmer.stem(i) for i in x]) #stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**here we detokenised our normalised tokens from stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenised_reviews)):\n",
    "    tokenised_reviews[i] = ' '.join(tokenised_reviews[i])\n",
    "    \n",
    "combi['clean_review'] = tokenised_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just don't get some of the big premises of t...</td>\n",
       "      <td>just some premis thi episod that miranda remar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Please note that I haven't seen the film since...</td>\n",
       "      <td>pleas note that haven seen film sinc discov to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I went for this movie believing it had good ra...</td>\n",
       "      <td>went thi movi believ good rate firstli ridicul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>First off, let me say that I am a great believ...</td>\n",
       "      <td>first that great believ fanpro stuff continu g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>OK i own this DVD i got it new at amazon... i ...</td>\n",
       "      <td>thi amazon mean think badass pretti cool flick...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews  \\\n",
       "0        0  I just don't get some of the big premises of t...   \n",
       "1        0  Please note that I haven't seen the film since...   \n",
       "2        0  I went for this movie believing it had good ra...   \n",
       "3        0  First off, let me say that I am a great believ...   \n",
       "4        0  OK i own this DVD i got it new at amazon... i ...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  just some premis thi episod that miranda remar...  \n",
       "1  pleas note that haven seen film sinc discov to...  \n",
       "2  went thi movi believ good rate firstli ridicul...  \n",
       "3  first that great believ fanpro stuff continu g...  \n",
       "4  thi amazon mean think badass pretti cool flick...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90,min_df=2,max_features=1000,\n",
    "                                 stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bow_vectorizer.fit_transform(combi['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus.*\n",
    "\n",
    "*TF-IDF works by penalising the common words by assigning them **lower weights while giving importance to words which are rare in the entire corpus** but appear in good numbers in few documents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectoriser = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000\n",
    "                                  ,stop_words='english')\n",
    "tfidf = tfidf_vectoriser.fit_transform(combi['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec features**\n",
    "\n",
    "*Word embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like* **King -man +woman = Queen, which is mind-blowing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of using word embeddings over BOW or TF-IDF are:\n",
    "\n",
    "1. Dimensionality reduction - significant reduction in the no. of features required to build a model.\n",
    "\n",
    "2. It capture meanings of the words, semantic relationships and the different types of contexts they are used in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Word2Vec is not a single algorithm but a combination of two techniques – **CBOW (Continuous bag of words) and Skip-gram model.** Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.*\n",
    "\n",
    "*CBOW tends to predict the probability of a word given a context. A context may be a single adjacent word or a group of surrounding words. The Skip-gram model works in the reverse manner, it tries to predict the context for a given word.*\n",
    "\n",
    "*There are three laters: - an input layer, - a hidden layer, and - an output layer.*\n",
    "\n",
    "*The input layer and the output, both are one- hot encoded of size [1 X V], where V is the size of the vocabulary (no. of unique words in the corpus). The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. The weights learned by the model are then used as the word-vectors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will go ahead with the **Skip-gram model** as it has the following advantages:\n",
    "\n",
    "1. It can capture two semantics for a single word. i.e it will have two vector representations of ‘apple’. One for the company Apple and the other for the fruit.\n",
    "\n",
    "2. Skip-gram with negative sub-sampling outperforms CBOW generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets train our own Wrod2vec\n",
    "\n",
    "tokenised_reviews= combi['clean_review'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.Word2Vec(tokenised_reviews,size =300,window=5,min_count = 2,sg = 1, hs = 0,\n",
    "             negative = 10, workers =2,seed =34)\n",
    "# size =300 #desired no. of features/ independent variables,\n",
    "#window=5, # context window size\n",
    "#min_count = 2,\n",
    "#sg = 1, #1 for skip-gram model\n",
    "#hs = 0,\n",
    "#negative = 10, # for negative sampling\n",
    "#workers =2, # no. of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71160675, 79784376)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(tokenised_reviews,total_examples = len(combi['clean_review']),epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pple', 0.6037256717681885),\n",
       " ('milkwoman', 0.5522492527961731),\n",
       " ('shiksa', 0.5226751565933228),\n",
       " ('finace', 0.5207064747810364),\n",
       " ('gona', 0.5180484652519226),\n",
       " ('deffin', 0.5136148929595947),\n",
       " ('snowqueen', 0.5132818818092346),\n",
       " ('herrera', 0.5066465139389038),\n",
       " ('fenni', 0.5040126442909241),\n",
       " ('espi', 0.5025908946990967)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unbox', 0.5081276893615723),\n",
       " ('ebay', 0.46306556463241577),\n",
       " ('luv', 0.4400131106376648),\n",
       " ('yippe', 0.43250173330307007),\n",
       " ('tinyurl', 0.4236803650856018),\n",
       " ('ofcors', 0.3953867256641388),\n",
       " ('ivar', 0.3872101306915283),\n",
       " ('tunnelvis', 0.38674068450927734),\n",
       " ('tassl', 0.3851141929626465),\n",
       " ('basin', 0.3839370012283325)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.04190290e-01, -1.65448681e-01, -1.50178581e-01,  2.94461906e-01,\n",
       "       -9.88465454e-03,  9.14824102e-03, -9.79987830e-02, -1.76489070e-01,\n",
       "        4.45896275e-02,  1.39640957e-01, -3.16530198e-01, -8.10668711e-03,\n",
       "        2.28770465e-01,  7.90330097e-02,  2.01814413e-01,  8.51582959e-02,\n",
       "        8.10376629e-02,  9.06000938e-03, -6.39278591e-02,  2.25870195e-03,\n",
       "        1.09640151e-01, -1.52539551e-01,  1.93866082e-02,  2.01116428e-01,\n",
       "       -2.95157619e-02,  1.73391446e-01,  2.46806800e-01,  4.86020260e-02,\n",
       "        3.44105363e-02, -2.16894403e-01, -3.97594087e-03,  2.87239458e-02,\n",
       "       -3.72260921e-02, -7.24124461e-02,  8.72854590e-02,  1.08643234e-01,\n",
       "       -1.69456765e-01, -3.55633572e-02, -6.02575205e-02,  7.64580145e-02,\n",
       "        2.02271104e-01,  2.85966605e-01,  7.41958944e-03, -1.43571049e-02,\n",
       "       -1.20622896e-01, -2.27686018e-01, -1.58697501e-01, -1.51734268e-02,\n",
       "        1.09189637e-01,  1.03441877e-02, -9.34014190e-03, -5.96045218e-02,\n",
       "        3.84941921e-02,  2.18962982e-01, -2.13022172e-01,  1.65266827e-01,\n",
       "       -7.16553186e-05, -2.09919930e-01, -6.37485534e-02, -1.47808060e-01,\n",
       "        1.65630039e-02,  1.27615348e-01,  3.61684151e-02,  1.19003870e-01,\n",
       "        1.06748424e-01,  1.27512604e-01,  9.66166854e-02,  8.25184286e-02,\n",
       "       -6.58928007e-02, -5.01115434e-02,  1.18535655e-02, -2.80392356e-02,\n",
       "        7.19133615e-02, -6.79398254e-02, -1.06145293e-01,  4.00743866e-03,\n",
       "       -1.67443171e-01,  1.22065365e-01, -1.57510638e-02,  1.07753746e-01,\n",
       "       -6.24123476e-02,  2.30058864e-01,  4.55545671e-02, -1.45516962e-01,\n",
       "        2.35450923e-01,  2.97440328e-02, -2.49928236e-01,  4.11732420e-02,\n",
       "        8.96532312e-02,  1.00475289e-01, -6.33948743e-02,  1.08721755e-01,\n",
       "       -5.08736409e-02,  8.49702880e-02,  1.35977101e-02,  1.75166622e-01,\n",
       "        1.16114505e-01,  1.31769702e-01,  1.72250699e-02, -1.54358586e-02,\n",
       "        4.62801084e-02, -1.00996727e-02,  1.12628073e-01,  7.44588226e-02,\n",
       "       -4.01436575e-02,  1.01531856e-01, -1.59323514e-01,  4.63023148e-02,\n",
       "        3.16549331e-01,  1.74506276e-03,  1.39022723e-01, -9.46155339e-02,\n",
       "        3.83707248e-02, -5.83518445e-02, -1.26285255e-02,  1.02742091e-01,\n",
       "        7.29295611e-02,  5.56212701e-02, -5.24240471e-02,  6.47104308e-02,\n",
       "        5.74212745e-02, -2.93674171e-02, -2.28123181e-02,  7.05801398e-02,\n",
       "        2.32688040e-01, -2.10252348e-02,  1.39222547e-01,  1.16725992e-02,\n",
       "       -5.82447834e-02,  2.38230098e-02, -1.18065126e-01,  3.48659395e-03,\n",
       "        2.58556306e-01,  2.75173336e-01, -5.15767857e-02,  2.21158788e-02,\n",
       "       -3.59163404e-01, -5.54934219e-02, -1.12102278e-01,  2.06277822e-03,\n",
       "       -1.65760994e-01, -2.05575258e-01,  2.38781631e-01, -7.04729483e-02,\n",
       "        1.11203119e-02, -3.54091227e-02, -1.83900222e-01,  3.04507792e-01,\n",
       "        8.42552185e-02,  2.58579999e-01,  4.77917343e-02,  2.21495271e-01,\n",
       "        1.49980754e-01, -2.04014927e-01, -2.86368690e-02,  3.89494039e-02,\n",
       "       -5.35086729e-02, -1.86350018e-01, -1.71862081e-01,  1.92879904e-02,\n",
       "        1.03953116e-01, -1.07490279e-01, -1.16298757e-01,  1.85032822e-02,\n",
       "        1.43307149e-01,  1.32430330e-01,  2.82435954e-01,  8.77978578e-02,\n",
       "       -5.57159409e-02, -4.25364375e-02, -4.38455939e-02, -6.97896108e-02,\n",
       "       -3.90922092e-02,  4.99036908e-02,  1.46700228e-02,  2.08571076e-01,\n",
       "        5.41431718e-02, -5.46138883e-02,  7.76849166e-02, -2.80834129e-03,\n",
       "       -1.54247001e-01, -1.54783249e-01,  1.11993045e-01, -3.91819961e-02,\n",
       "        2.63100833e-01,  6.05030134e-02,  1.83587790e-01,  1.76490977e-01,\n",
       "        1.29899457e-01, -8.81123617e-02, -4.78455983e-02,  1.61783174e-01,\n",
       "       -6.40839636e-02,  1.46750659e-01,  4.32710312e-02, -7.93064460e-02,\n",
       "       -2.05034390e-02,  1.38461605e-01, -9.75941718e-02, -1.20128756e-02,\n",
       "       -2.51749098e-01,  8.88520256e-02,  4.98602800e-02,  8.71829242e-02,\n",
       "       -1.03325762e-01, -1.17712528e-01, -6.94692805e-02, -2.03076661e-01,\n",
       "       -1.70935988e-02, -2.35259309e-02, -1.88906223e-01, -1.24436855e-01,\n",
       "        1.84834808e-01,  3.25605758e-02,  1.83990374e-02, -2.59170830e-01,\n",
       "        4.67851907e-02,  1.91819612e-02, -6.14762604e-02,  3.65222096e-02,\n",
       "       -9.59513038e-02,  4.03165892e-02, -1.58610612e-01, -3.57189067e-02,\n",
       "       -6.23927452e-02, -1.81135282e-01, -2.76543368e-02, -4.29169498e-02,\n",
       "       -1.14477584e-02,  1.00016609e-01, -7.79767334e-02,  2.90253013e-02,\n",
       "       -4.38993014e-02,  1.29630361e-02,  8.27047005e-02,  3.19197141e-02,\n",
       "        1.55293271e-01,  4.11834242e-03,  1.69156678e-02,  3.85823776e-03,\n",
       "       -2.65692901e-02,  6.26947358e-02,  1.35580659e-01,  9.28568989e-02,\n",
       "       -1.76846255e-02,  7.78949782e-02,  1.67132486e-04, -4.45999280e-02,\n",
       "        1.18859142e-01,  4.52846318e-04, -5.26950061e-02,  1.54029742e-01,\n",
       "       -8.03062916e-02, -2.77065247e-01,  1.45199880e-01,  1.15277387e-01,\n",
       "       -9.70311183e-03, -1.97462458e-02,  7.23752081e-02,  1.34929314e-01,\n",
       "        2.42764037e-02, -2.39806995e-01,  1.10689767e-01, -8.20398554e-02,\n",
       "       -6.93236440e-02, -7.32013909e-03, -5.46968430e-02, -2.25737989e-01,\n",
       "        6.99437708e-02, -1.91657573e-01, -7.49423914e-03,  1.88457832e-01,\n",
       "        1.99650303e-01, -5.99800795e-02, -7.39756525e-02, -6.45293966e-02,\n",
       "        1.44500554e-01,  2.11210221e-01,  1.04824975e-01, -1.63610056e-01,\n",
       "       -4.40903008e-02,  9.73336622e-02,  1.47489443e-01, -8.07482284e-03,\n",
       "       -5.39896488e-02,  1.18266940e-01, -7.26561323e-02,  1.14289694e-01,\n",
       "       -1.17277451e-01,  3.60276215e-02, -1.52034566e-01,  3.09257451e-02,\n",
       "       -2.66149081e-02, -3.63742970e-02, -3.43212485e-02,  1.75771043e-01,\n",
       "       -4.21111658e-02, -1.21440887e-01, -1.24847777e-01,  5.03520928e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector reprensentation of any word in our corpus.\n",
    "model_w2v['movi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v['movi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data contains tweets and not just words, we’ll have to figure out a way to use the word vectors from word2vec model to create vector representation for an entire tweet. There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. The length of the resultant vector will be the same, i.e. 200. We will repeat the same process for all the tweets in our data and obtain their vectors. Now we have 200 word2vec features for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens,size):\n",
    "    vec = np.zeros(size).reshape((1,size))\n",
    "    count =0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1,size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            if(count !=0):\n",
    "                vec /=count\n",
    "    return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenised_reviews),300))\n",
    "\n",
    "for i in range(len(tokenised_reviews)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenised_reviews[i],300)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.save('gensimWord2vec.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now after creating every feature we have to train the model with each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling our train and test data set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spliting the data. for the bag of words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:25000,:] #training set of bof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = bow[25000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainBow, XValBow, yTrainBow, yValBow = train_test_split(train_bow,df['ratings'], random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(XTrainBow,yTrainBow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionBow =  logreg.predict_proba(XValBow) \n",
    "# taking prediction on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionIntBow = predictionBow[:,1] >=0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionIntBow = predictionIntBow.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8442513368983958"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yValBow,predictionIntBow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f1 score of 84 percent thats a nice start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using TF-IDF features in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTFIDF = tfidf[:25000,:]\n",
    "# training set\n",
    "testTFIDF = tfidf[25000:,:]\n",
    "#test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting training and test data set into val, xtrain, ytrain.\n",
    "#Xtrain\n",
    "XTrainTFIDF = trainTFIDF[yTrainBow.index]\n",
    "#XVal\n",
    "XValTFIDF = trainTFIDF[yValBow.index]\n",
    "#yTrain\n",
    "yTrainTFIDF = yTrainBow\n",
    "#yval\n",
    "yValTFIDF = yValBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(XTrainTFIDF,yTrainTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTFIDF = logreg.predict_proba(XValTFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictIntTFIDF = predictionTFIDF[:,1]>=0.33\n",
    "predictIntTFIDF = predictIntTFIDF.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8459450656823364"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yValTFIDF,predictIntTFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slightly greater accuracy then previous one.i.e 84.59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainW2V = wordvec_df.iloc[:25000,:]\n",
    "testW2V = wordvec_df.iloc[25000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data into x train and y train and validation set.\n",
    "XTrainW2V = trainW2V.iloc[yTrainBow.index,:]\n",
    "\n",
    "# X_val set.\n",
    "XValW2V = trainW2V.iloc[yValBow.index,:]\n",
    "\n",
    "#ytrain\n",
    "yTrainW2V = yTrainBow\n",
    "\n",
    "#yVal \n",
    "yValW2V = yValBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chauhans\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(XTrainW2V,yTrainW2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionW2V = logreg.predict_proba(XValW2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictIntW2V = predictionW2V[:,1] >=0.33\n",
    "predictIntW2V = predictIntW2V.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8341098710586197"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yValW2V,predictIntW2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### here accuracy is slightly less than above features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
